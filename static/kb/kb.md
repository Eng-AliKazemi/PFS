# Precision File Search (PFS) - The Complete Knowledge Base

This document is the official user manual and technical reference for the Precision File Search application. It covers every feature, setting, and concept in detail, with practical examples to guide you.

## Table of Contents
1.  [Project Purpose ](#1-project-purpose--vision)
2.  [AI Search](#2-ai-search)
3.  [Semantic Search](#3-semantic-search)
4.  [Classic Search](#4-classic-search)
5.  [Document Classifier](#5-document-classifier)
6.  [Application Security](#6-application-security)
7.  [Architecture & Design Patterns](#7-architecture--design-patterns)
8.  [ML Deep Dive: Classifier Engine](#8-ml-deep-dive-classifier-engine)
9.  [RAG Pipeline Deep Dive](#9-rag-pipeline-deep-dive)
10. [Settings & Configuration](#10-settings--configuration)
11. [File Handling & Best Practices](#11-file-handling--best-practices)
12. [License Agreement](#12-license-agreement)

---

## 1. Project Purpose 

Precision File Search (PFS) was conceived to solve the everyday chaos of digital file management. It was built to address real-world pain points with a comprehensive, innovative, and modern approach not found in other single applications.

#### Core Problems Solved:
*   **Fast File & Folder Search:** Instantly locate files and folders, even those buried deep inside nested subdirectories, with lightning-fast performance. No indexing required, making it ideal for dynamic or frequently changing environments.
*   **Instant Keyword Search:** Specific keywords can be found inside documents scattered across drives and sub-folders, without a pre-indexing process.
*   **Semantic & Contextual Search:** Beyond keywords, documents can be found based on meaning and concepts, uncovering information that might otherwise be difficult to find.
*   **Adaptive Natural Language Search:** Users can type questions or phrases in plain English, no special syntax required. The AI automatically interprets the intent and selects the most effective search method, whether it's a classic keyword match or a more nuanced semantic search based on meaning and context. This ensures the most relevant results, even when the user doesn‚Äôt know the exact terms to look for.
*   **Natural Language Insights:** Complex questions can be asked in plain English to receive summarized answers with cited sources, turning files into a conversational knowledge base.
*   **Automated Organization:** A messy collection of scattered files can be automatically sorted into clean, organized categories with a single click.

#### Guiding Principles:
*   **Accessible to All Users:** PFS was designed with a clean, modern, and intuitive user interface, hiding immense technical complexity to make its powerful features accessible to all users, not just developers.
*   **Privacy-First & Offline:** File privacy is a core principle. The application runs entirely on the local machine. File contents are not sent to any external servers*, making it a secure tool that aligns with the principles of GDPR and HIPAA.
*   **Built for the Future:** Engineered with a modular architecture, PFS is designed to be easily maintained, upgraded, and extended with new features.

<small><em>*The only exception is the LLM provider which can be optionally configured by the user for the AI Search feature.</em></small>

---

## 2. AI Search

In the Precision File Search (PFS) application, AI Search is the primary and most powerful interface for interacting with files. It allows natural language to be used to ask complex questions, find documents, and get summarized answers.

#### How it Works:
1.  **Intent Routing:** The AI first determines the query's intent. If a question is asked about the app, the internal knowledge base is used. If a request is made to find local files, the file search pipeline is initiated. You can ask about any part of the application, including full documentation, technical details, and licensing, by starting your query with **PFS: + QUERY**
2.  **File Search Pipeline:** For file searches, the best tool is determined: a fast "Classic Search" for filenames, or a deep "Semantic Search" for conceptual content.
3.  **Synthesis and Response:** After information is retrieved, a final, summarized answer is generated by the AI in Markdown, complete with source files.

#### Best Use Example:
`Example: PFS: How semantic search works?`
`What was the net profit for Project Titan in Q3?`

The AI will use semantic search, find the files or knowledge, read the content, extract the exact number, and present it as the answer.

---

## 3. Semantic Search

In the Precision File Search (PFS) application, the Semantic Search tab gives direct access to the advanced semantic engine, allowing files to be searched based on meaning and context, not just keywords.

#### How to Use:
1.  **Build Index:** A directory path is provided and "BUILD / UPDATE INDEX" is clicked. The first time you build an index, the engine processes all supported files. On subsequent updates, the **intelligent re-indexing system** automatically detects only new or modified files, making the process significantly faster while ensuring your knowledge base is always up-to-date.
2.  **Search:** Once the index is ready, conceptual questions can be asked to find the most relevant sections within documents.

#### Understanding the Parameters:
*   **Initial Candidates (K):** The number of potential results to pull in the first pass.
*   **Vector Score Threshold:** The minimum similarity score for a result to be considered.
*   **Enable Reranker (Recommended):** A powerful model is used to re-analyze and re-order the initial results for much higher accuracy.

---

## 4. Classic Search

In the Precision File Search (PFS) application, Classic Search is the traditional search tool, now powered by a **High-Performance File Engine** designed for maximum speed and efficiency. It uses a concurrent architecture to parallelize file system operations, allowing it to remain responsive while scanning thousands of files per second.

#### Core Engine Features:
*   **Concurrent Processing:** The engine uses multiple threads to read and search files simultaneously, making full use of modern hardware to finish scans faster.
*   **Memory Efficiency:** Large files are read in small, manageable chunks. This ensures that memory usage remains low and stable, preventing crashes even when scanning multi-gigabyte files.
*   **Optimized Matching:** Multi-keyword searches are compiled into a single, efficient pattern, allowing for rapid scanning in one pass.

#### Search Types:
*   **Keywords in Files:** The text content inside files is scanned.
*   **File Names:** Only filenames are searched. Supports wildcards like `*`.
*   **Folder Names:** Only the names of folders are searched.
*   **Filter by Category:** Searches can be refined using predefined categories such as Documents, Pictures, Videos, Audio, Compressed and Executable.

#### Search Management:
The **Saved Searches** and **Search History** panels can be used to save and quickly re-run complex queries.

---

## 5. Document Classifier

In the Precision File Search (PFS) application, the Classifier uses a pre-trained Machine Learning model to automatically sort files into predefined categories like "Resume", "Invoice", "Report", etc.

#### How it Works:
1.  A directory path is provided and "START CLASSIFICATION" is clicked.
2.  Each file is scanned and a category is predicted.
3.  Results are grouped by tag. Manual file operations (**Copy**, **Move**, **Delete Entry**) can be performed on each category individually.

#### One-Click Auto Organization
For maximum efficiency, after a classification is complete, a new **"Auto Organize"** button will appear above the results. This feature streamlines the entire sorting process:
1.  The **"Auto Organize"** button is clicked.
2.  A single base destination folder is provided (e.g., `C:\Organized Documents`).
3.  Sub-folders for each tag (e.g., `\Invoices`, `\Resumes`) are automatically created inside the destination.
4.  All classified files are then moved into their corresponding new folders in one operation, clearing the results list.

This turns a multi-step manual process into a single-click action, perfect for quickly sorting large, scattered archives.

#### Model Training: Building Your Custom Classifier
In the **Settings** tab, you have the power to re-train the document classification model using your own data. This allows you to move beyond the default categories and create a classifier perfectly tailored to your specific needs, whether for sorting legal documents, academic papers, or project files.

The performance of your custom model is **heavily dependent on the quality and structure of your training data**. Follow this guide carefully to achieve the best results.

**Step 1: Preparing Your Training Data - The Blueprint for Success**
The training process requires a specific directory structure. The model learns by example, associating the content of files with the name of the folder they are in.

The structure is simple: a main parent folder containing one sub-folder for each category you want to create.
1.  **Create a Main Training Folder:** On your computer, create a new folder. This will be your main data directory. Let's call it `My-Training-Data`.
2.  **Create Category Sub-Folders:** Inside `My-Training-Data`, create one folder for each category you want the model to learn. The **name of each folder** will become the **category label** in the application.
3.  **Add Your Documents:** Place at least 15-20 example documents into each category sub-folder. These are the "textbooks" the model will study to learn what defines that category.

**Visual Example:**
Imagine you are a researcher and want to classify documents into three categories: `Climate-Studies`, `Economic-Reports`, and `Policy-Briefs`. Your directory structure **must** look like this:
```
D:\My-Training-Data\
‚îú‚îÄ‚îÄ Climate-Studies\
‚îÇ   ‚îú‚îÄ‚îÄ arctic_melt_rates.pdf
‚îÇ   ‚îú‚îÄ‚îÄ ocean_acidification_2024.docx
‚îÇ   ‚îú‚îÄ‚îÄ report_on_amazon_deforestation.txt
‚îÇ   ‚îî‚îÄ‚îÄ ... (at least 15-20 total files)
‚îÇ
‚îú‚îÄ‚îÄ Economic-Reports\
‚îÇ   ‚îú‚îÄ‚îÄ Q3_gdp_analysis.xlsx
‚îÇ   ‚îú‚îÄ‚îÄ inflation_trends_memo.docx
‚îÇ   ‚îú‚îÄ‚îÄ market_summary.pdf
‚îÇ   ‚îî‚îÄ‚îÄ ... (at least 15-20 total files)
‚îÇ
‚îî‚îÄ‚îÄ Policy-Briefs\
    ‚îú‚îÄ‚îÄ healthcare_reform_proposal.pdf
    ‚îú‚îÄ‚îÄ summary_of_energy_bill.docx
    ‚îú‚îÄ‚îÄ notes_on_urban_planning.txt
    ‚îî‚îÄ‚îÄ ... (at least 15-20 total files)
```
When you point the trainer to the `D:\My-Training-Data` folder, it will automatically understand that it needs to learn three labels: `Climate-Studies`, `Economic-Reports`, and `Policy-Briefs`.

**Step 2: Tips for Training the Best Possible Model**
Merely having the right structure isn't enough. Follow these professional tips to ensure your model is accurate and reliable.
*   **Tip 1: Quality Over Quantity.** Clean, relevant examples are far better than thousands of messy ones. Ensure the documents in each category are truly representative of that category. A single misfiled document can confuse the model.
*   **Tip 2: Balance Your Dataset.** Try to have a roughly equal number of documents in each category. If you have 1000 `Invoices` but only 10 `Resumes`, the model may become biased and lazy, tending to guess "Invoice" more often because it's statistically safer. A balanced dataset forces it to learn the real differences.
*   **Tip 3: Ensure Categories are Distinct.** The model learns from the unique vocabulary within your documents. If two categories are too similar, the model will struggle. For example, training on `Contracts` and `Agreements` might be difficult if the documents use very similar legal language. The more unique the keywords and phrases in each category, the higher the model's accuracy will be.
*   **Tip 4: Provide Enough Data to Learn.** While there is no magic number, a good starting point is **at least 15-20 documents per category**. For highly complex topics, more is better (50+). You need enough examples for the model to identify statistically significant patterns.
*   **Tip 5: Understanding the Training Parameters in Settings.** You have two sliders to fine-tune the training process:
    *   **Test/Train Split:** This determines what percentage of your data is held back to "test" the model after training. A value of 20% means the model trains on 80% of your files and is then given a final exam on the 20% it has never seen before. The final accuracy score reflects its performance on this unseen data.
    *   **Number of Estimators (Trees):** The model (a RandomForest) is like a committee of decision-making "trees." More trees can lead to a more accurate model, but it will also significantly increase the training time. The default of `100` is a good balance, but you can increase it to `200` or `300` for potentially better results on complex datasets.

**Step 3: Running the Training and Interpreting Results**
1.  **Navigate:** Go to the **Settings** tab in the application.
2.  **Locate:** Scroll down to the **Classifier Model Training** section.
3.  **Input Path:** Enter the full path to your main training folder (e.g., `D:\My-Training-Data`).
4.  **Adjust Parameters:** Set the Test/Train split and Number of Estimators as desired.
5.  **Start Training:** Click the **START TRAINING** button.
6.  **Monitor:** The "Training Progress" box will appear, showing you a live log of the process: loading categories, splitting data, training, and evaluating.
7.  **Check Accuracy:** When complete, a **Final Model Accuracy** will be displayed. An accuracy of **85% or higher** is generally considered very good and indicates you have a reliable model. If your accuracy is low, revisit the tips in Step 2.

After the training is complete, the new `document_classifier.ml` file is saved. **You must restart the application** for it to load and use your new custom model.

---

## 6. Application Security

The Precision File Search (PFS) application is designed from the ground up to be a secure, locally-run tool. Modern application security best practices are followed to protect user data and system integrity.

#### Core Security Principles:
*   **Local First:** The server is bound to `127.0.0.1` (localhost) by default. This is the single most important security feature, as it prevents any external computers on the network from accessing the application.
*   **No Data Exfiltration:** File contents or search queries are not sent to any external servers, with the sole exception of the LLM provider configured for AI Search.

#### Built-in Security Features:
*   **SQL Injection Prevention**
    **The Vulnerability:** An attacker provides malicious input that tricks the application into running unintended database commands, potentially leading to data theft or corruption.
    **The Prevention Method:** All database queries use parameterized statements. This industry-standard technique separates the command from the data, making it impossible for user input to be executed as a command.

*   **Prompt Injection Hardening**
    **The Vulnerability:** Malicious instructions hidden inside files or user queries manipulate the AI into ignoring its primary goal, potentially causing it to generate harmful content or reveal its internal programming.
    **The Prevention Method:** The prompts sent to the AI are engineered with defensive instructions, warning it about untrusted content and reinforcing its core mandate. This helps the AI to distinguish between its mission and malicious user data.

*   **Path Traversal Prevention**
    **The Vulnerability:** A*   **Path Traversal Prevention**
    *   **The Vulnerability:** A malicious query attempts to trick the application into accessing or modifying sensitive system files outside of the intended directory by using relative path components (e.g., `../../../../Windows/System32`).
    *   **The Prevention Method:** Before any file system operation, the application validates and **canonicalizes** the target path. This process safely resolves any relative path components (like `..`), transforming the input into a direct, absolute path. This effectively blocks any attempt to "climb" out of the requested directory, ensuring that only the explicitly specified folder is ever accessed.

*   **Cross-Site Scripting (XSS) Prevention**
    **The Vulnerability:** XSS occurs when an application renders unsanitized data, allowing an attacker to inject malicious scripts that execute in the user's browser. This could steal API keys or perform unauthorized actions.
    **The Prevention Method:** The application employs a two-layered defense. First, all AI-generated content is passed through a security sanitizer (DOMPurify) to strip out any malicious code before it is displayed. Second, all file and folder paths shown in classic search results are strictly escaped to ensure they are rendered as plain text, not executable code.

*   **Regular Expression Denial of Service (ReDoS) Mitigation**
    **The Vulnerability:** ReDoS is an attack where a specially crafted regular expression can cause an application to hang or crash by consuming excessive CPU resources.
    **The Prevention Method:** This is mitigated by running the regular expression matching in a separate, non-blocking process. This isolates the potentially slow operation, ensuring that even a malicious pattern cannot freeze the main application server or the user interface.

*   **Information Disclosure Prevention**
    **The Vulnerability:** This can occur when an application's error messages reveal sensitive internal details, such as full file paths or configuration values, which could aid an attacker.
    **The Prevention Method:** The application is designed to catch all backend exceptions. It logs the full, detailed error server-side for debugging but only sends a generic, safe error message to the user's browser, preventing any sensitive information from being leaked.

---
#### User Responsibility & Disclaimer
Like any tool, this application can be misused. The primary security responsibility lies with the user. Since this application runs locally and has access to the file system, granting physical access to the machine running this software to an untrusted individual could lead to a security breach.

Therefore, the application is provided "as-is" without any warranty of any kind, express or implied. In no event shall the developer be liable for any claim, damages, or other liability arising from the use of the software.

However, the developer is committed to maintaining and improving the security of the application. Security issues will be followed up on seriously, and patches will be provided in future versions.

---

## 7. Architecture & Design Patterns

The Precision File Search (PFS) project represents a deliberate synthesis of five critical technology disciplines, creating a combined solutions architecture that addresses real-world problems in an innovative and intuitive form. It is a showcase of best practices from:
*   **Software Engineering:** Built on a robust, modular, and asynchronous Python (FastAPI) backend to ensure high performance and maintainability.
*   **Data Science:** Features a classic, trainable machine learning model (Scikit-learn) for automatically classifying and organizing unstructured documents.
*   **Artificial Intelligence:** Leverages a state-of-the-art RAG pipeline (LangChain) and Large Language Models to enable conversational, context-aware search and data synthesis.
*   **User Experience (UX):** Designed to hide immense technical complexity behind a simple, clean, and intuitive JavaScript and CSS frontend.
*   **Application Security:** Engineered from the ground up with security-first principles like path validation, XSS sanitization, parameterized queries, and more.

#### Core Design Patterns:
*   **AI Orchestrator:** The AI Search acts as an intelligent controller that routes user requests through a chain of: Intent Routing ‚Üí Strategy Selection ‚Üí Data Retrieval ‚Üí Final Summarization.
*   **Two-Stage Retrieval (Recall & Precision):** The semantic engine uses a fast vector search to "recall" candidates, followed by a powerful reranker to ensure "precision" of the final results.
*   **Parent-Child Chunking:** Documents are split into large "parent" chunks (for context) and small "child" chunks (for precise searching).
*   **Centralized, Persistent Configuration:** All application settings are managed in a dedicated SQLite database, ensuring consistency and persistence.
*   **Centralized Logging System:** A robust logging module provides structured, level-based logs to both the console and rotating files for easy debugging and monitoring.

#### Technology Stack:
*   **Backend:** FastAPI, Uvicorn, LangChain.
*   **Data Storage:** Qdrant (Vector DB), SQLite (Document Store & Config).
*   **NLP Models:** Hugging Face Transformers for embeddings and reranking.
*   **ML Model:** Scikit-learn Random Forest, A supervised machine learning classifer.
*   **AI Models:** Offline or Online, Open-source or Proprietary.

---

## 8. ML Deep Dive: Classifier Engine

The Document Classifier is powered by a classic supervised machine learning model: the **RandomForestClassifier** from the **Scikit-learn** library.

#### Why This Model Was Chosen:
*   **Balance of Speed and Accuracy:** Random Forest offers excellent performance for text classification tasks without requiring the heavy computational resources (like a GPU) that deep learning models need and it is fully offline.
*   **Robustness:** By averaging the predictions of many individual decision trees, it is less prone to overfitting and generally more stable than single models.
*   **Accessibility:** It can be trained effectively on a standard CPU, making it perfect for a locally-run application.

#### How It Works:
1.  **Text to Numbers (TF-IDF):** The model first uses a `TfidfVectorizer`. This crucial step turns the text content of a document into a numerical signature. It does this by identifying which words are most important and unique to that document compared to all other documents.
2.  **The 'Forest' of Trees:** The model then builds a "forest" of many individual "decision trees." Each tree is like a flowchart of questions about the words in the document (e.g., "Does it contain the word 'invoice' more than 3 times?").
3.  **Voting for a Category:** Each tree in the forest independently "votes" for a category. The final prediction is the category that receives the most votes, a process which makes the model highly reliable.

---

## 9. RAG Pipeline Deep Dive

The Semantic and AI Search features are powered by a **Retrieval-Augmented Generation (RAG)** pipeline. This is a state-of-the-art AI technique that combines factual knowledge from documents with the reasoning ability of a Large Language Model (LLM).

#### How the Pipeline Works:
1.  **Indexing (The 'Library'):** When an index is built, documents are processed:
    *   **Hugging Face Embeddings:** A specialized model converts each text chunk into a "vector", a numerical representation of its semantic meaning.
    *   **Qdrant (Vector Database):** This high-speed database stores these vectors, creating a multi-dimensional "map" of the documents' meaning that can be searched instantly.
2.  **Retrieval (The 'Search'):** When a question is asked, the two-stage retrieval process begins:
    *   **1. Vector Search (Recall):** The query is converted into a vector. Qdrant rapidly finds the document chunks with the most similar vectors. This is a broad but fast initial search.
    *   **2. Reranking (Precision):** A more powerful "Cross-Encoder" model from Hugging Face takes these initial results and does a deep, pairwise comparison against the original query, re-scoring and re-ordering them for maximum relevance.
3.  **Generation (The 'Answer'):**
    *   **LangChain:** This framework acts as the "conductor." It takes the top-ranked, relevant chunks from the reranker and packages them into a carefully engineered prompt along with the original question.
    *   **LLM Synthesis:** This complete package is sent to the configured LLM (e.g., LM Studio, Ollama, Groq, OpenAI). The LLM uses the provided text as its sole source of truth to generate a final, factual, and synthesized answer.

#### Benefits & Proper Usage:
This approach dramatically reduces AI "hallucinations" by forcing the model to base its answers on actual data. For best results, the **AI Search** tab should be used to ask conceptual, natural language questions (e.g., "Summarize the Q4 marketing strategy" instead of "last month marketing").

---

## 10. Settings & Configuration

In the Precision File Search (PFS) application, the Settings tab allows every component of the application to be fine-tuned. Note that changes to models or the default Reranker state require an application restart to take effect.

#### File Scan Settings
*   **Excluded Folders:** A comma-separated list of folder names (e.g., `node_modules, .git`) to ignore during all search and indexing operations.
*   **File Extensions:** Defines which file types will have their content read. This affects both "Keywords in Files" searches and Semantic Indexing.

#### LLM Settings
These settings are required for the **AI Search** feature to function. They connect the application to a Large Language Model provider.
*   **API Key, Model Name, API Base URL:** Credentials for the LLM service (e.g., LM Studio, Ollama, Groq, OpenAI).

#### Advanced Retrieval Settings
These parameters control the behavior of the Semantic Search engine, allowing a balance between speed and accuracy.
*   **Enable Reranker by Default:** If checked, the powerful (but memory-intensive) reranker model is loaded on application startup. This provides the highest quality search results.
*   **The Search Funnel:** The retrieval process can be thought of as a funnel. It starts wide and progressively narrows down to the best results:
    1.  **Initial Candidates (K):** A wide net is cast, retrieving this many documents in the first fast search.
    2.  **Vector Score Threshold:** The first filter, keeping only documents with a basic similarity score above this value.
    3.  **Rerank Score Threshold:** The second, stricter filter applied after the expert reranker model has re-scored the documents.
    4.  **Final Results to Display (Top-N):** The final number of top-scoring documents to show in the UI.

#### Classifier Model Training
These settings control the retraining process for the Document Classifier.
*   **Test/Train Split:** The percentage of the data set aside to test the model's accuracy after it has been trained.
*   **Number of Estimators:** Controls the complexity of the machine learning model. More estimators can increase accuracy but will also increase training time.

---

## 11. File Handling & Best Practices

The Precision File Search (PFS) application's ability to read and index file content is crucial for all its features. Understanding how different file types are handled will help achieve the best results.

#### Optimizing the Search Index
While the app can technically process many file types, for the highest quality search results, the focus should be on files with rich text content. The following master list is curated for this purpose. Extensions can be added or removed from this list in the **Settings** tab to control what gets indexed.

`.txt` `.md` `.pdf` `.docx` `.doc` `.rtf` `.log` `.csv` `.json` `.xml` `.eml` `.msg` `.epub` `.html` `.htm` `.rst` `.odt` `.wpd` `.pages` `.xlsx` `.xls` `.pptx` `.ppt` `.ods` `.odp` `.tsv` `.tex` `.py` `.js` `.java` `.c` `.cpp` `.h` `.hpp` `.cs` `.go` `.rb` `.rs` `.swift` `.kt` `.scala` `.php` `.pl` `.vb` `.css` `.scss` `.sass` `.less` `.svg` `.jsx` `.tsx` `.yaml` `.yml` `.ini` `.toml` `.sql` `.conf` `.cfg` `.env` `.properties` `.sh` `.bash` `.ps1` `.bat`

#### Tips for Different File Types
*   **Emails (.eml, .msg):** Perfect for indexing. The entire email body, headers (To, From, Subject), and text-based attachments are made searchable.
*   **Spreadsheets (.xlsx, .xls):** The text content from all cells across all sheets is extracted and combined. Searches will be most effective when looking for specific data points, names, or terms known to be within the spreadsheet.
*   **Presentations (.pptx, .ppt):** Text from all slides and from the "speaker notes" section is indexed. This makes it easy to find an entire presentation based on a single bullet point or note.
*   **Code & Config (.py, .json, .yml):** These are treated as pure text, making the search engine a powerful tool for developers looking for specific functions, API keys, or configuration settings across a large codebase.
*   **A Note on Scanned PDFs & Images:** The application does **not** currently perform Optical Character Recognition (OCR). This means text within image files (.jpg, .png) or PDFs that are scans of paper documents will not be read. For a PDF to be searchable, its text must be selectable.

#### Recommended Excluded Folders
To ensure fast and relevant searches, it is highly recommended that the following folders be added to the "Excluded Folders" list in Settings. This prevents the engine from scanning millions of unnecessary files.

##### Core Recommendations (For All Users)
`$RECYCLE.BIN` `System Volume Information` `.Trash` `.Trashes` `AppData` `Application Data` `Library` `.cache` `cache` `logs` `tmp` `temp`

##### Extended List (For Developers & Power Users)
`node_modules` `.git` `.svn` `.hg` `dist` `build` `out` `target` `bin` `obj` `venv` `.venv` `env` `.env` `__pycache__` `.vscode` `.idea` `.vs` `vendor` `bower_components` `.npm` `.nuget` `.gradle` `.project` `.settings` `.classpath` `.pytest_cache`

---

## 12. License Agreement

### üìÑ Precision File Search & Classifier Non-Commercial License v1.0

#### Preamble
This license governs the use of the Precision File Search & Classifier software, developed by **Ali Kazemi**. It is designed to protect the project's integrity while encouraging non-commercial and educational use.

#### Terms and Conditions
1.  **Non-Commercial Use:** Use of the software and its source code for any non-commercial purpose is **PERMITTED**. This includes internal use in commercial entities, provided it does not generate revenue.
2.  **Personal and Educational Modification:** Modification of the source code for personal or educational purposes is **PERMITTED**.
3.  **Non-Commercial Distribution:** Distribution is **PERMITTED** provided it is non-commercial and includes attribution to the original author and a copy of this license.
4.  **Commercial Use and Sale:** Any form of commercial exploitation is strictly **PROHIBITED** without prior written permission.
5.  **License Inclusion:** Any distribution **must include a full copy of this license agreement**.

#### Disclaimer of Warranty
THE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND. IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES, OR OTHER LIABILITY.

#### Contact Information
**Developer**: Ali Kazemi | **LinkedIn**: [linkedin.com/in/Eng-AliKazemi](https://www.linkedin.com/in/Eng-AliKazemi/) | **Email**: eng.ali.kazemi@outlook.com